<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>数值稳定性和模型初始化 | LiuShuixiuのBlog</title>
    <meta name="generator" content="VuePress 1.9.9">
    <link rel="icon" href="/img/logo.webp">
    <meta name="description" content="关山难越，谁悲失路之人。萍水相逢，尽是他乡之客">
    <meta name="keywords" content="前端博客,个人技术博客,前端,前端开发,前端框架,web前端,前端面试题,技术文档,学习,面试,JavaScript,js,ES6,TypeScript,vue,python,css3,html5,Node,git,github,markdown">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/assets/css/0.styles.a632c044.css" as="style"><link rel="preload" href="/assets/js/app.e2570dae.js" as="script"><link rel="preload" href="/assets/js/2.c541ba58.js" as="script"><link rel="preload" href="/assets/js/3.4de99684.js" as="script"><link rel="preload" href="/assets/js/19.b97bc82f.js" as="script"><link rel="prefetch" href="/assets/js/10.d246a05f.js"><link rel="prefetch" href="/assets/js/100.d91e3bd6.js"><link rel="prefetch" href="/assets/js/101.29310af4.js"><link rel="prefetch" href="/assets/js/102.394b081e.js"><link rel="prefetch" href="/assets/js/103.a84d2be0.js"><link rel="prefetch" href="/assets/js/104.6e31fdb2.js"><link rel="prefetch" href="/assets/js/105.d29540db.js"><link rel="prefetch" href="/assets/js/106.4444c1ec.js"><link rel="prefetch" href="/assets/js/107.1a126f31.js"><link rel="prefetch" href="/assets/js/108.fab1bef3.js"><link rel="prefetch" href="/assets/js/109.7da74ed5.js"><link rel="prefetch" href="/assets/js/11.7cec26db.js"><link rel="prefetch" href="/assets/js/110.4c821873.js"><link rel="prefetch" href="/assets/js/111.16b2a267.js"><link rel="prefetch" href="/assets/js/112.db1498d5.js"><link rel="prefetch" href="/assets/js/113.430c6f8d.js"><link rel="prefetch" href="/assets/js/114.4d7cd289.js"><link rel="prefetch" href="/assets/js/115.6e210376.js"><link rel="prefetch" href="/assets/js/116.d55c838b.js"><link rel="prefetch" href="/assets/js/117.d9e4d91a.js"><link rel="prefetch" href="/assets/js/118.1bea0a85.js"><link rel="prefetch" href="/assets/js/12.05d9f898.js"><link rel="prefetch" href="/assets/js/13.8ca37ad0.js"><link rel="prefetch" href="/assets/js/14.ba6f4aa5.js"><link rel="prefetch" href="/assets/js/15.24d40c9d.js"><link rel="prefetch" href="/assets/js/16.a9725d5c.js"><link rel="prefetch" href="/assets/js/17.ef247b40.js"><link rel="prefetch" href="/assets/js/18.7d77bb9e.js"><link rel="prefetch" href="/assets/js/20.107b7e7a.js"><link rel="prefetch" href="/assets/js/21.94f2ca18.js"><link rel="prefetch" href="/assets/js/22.6579c0e1.js"><link rel="prefetch" href="/assets/js/23.a6fd4c75.js"><link rel="prefetch" href="/assets/js/24.ae0115b9.js"><link rel="prefetch" href="/assets/js/25.e52b4214.js"><link rel="prefetch" href="/assets/js/26.76fb7a05.js"><link rel="prefetch" href="/assets/js/27.197d345e.js"><link rel="prefetch" href="/assets/js/28.c640c895.js"><link rel="prefetch" href="/assets/js/29.85bae48e.js"><link rel="prefetch" href="/assets/js/30.4bfc191a.js"><link rel="prefetch" href="/assets/js/31.80dd0c3e.js"><link rel="prefetch" href="/assets/js/32.647925ba.js"><link rel="prefetch" href="/assets/js/33.ea6a33c3.js"><link rel="prefetch" href="/assets/js/34.6eea47bf.js"><link rel="prefetch" href="/assets/js/35.cd04abdc.js"><link rel="prefetch" href="/assets/js/36.e30518b4.js"><link rel="prefetch" href="/assets/js/37.bdad5d02.js"><link rel="prefetch" href="/assets/js/38.801efcb5.js"><link rel="prefetch" href="/assets/js/39.f7393434.js"><link rel="prefetch" href="/assets/js/4.89e39143.js"><link rel="prefetch" href="/assets/js/40.83064c02.js"><link rel="prefetch" href="/assets/js/41.281ccede.js"><link rel="prefetch" href="/assets/js/42.dbab7991.js"><link rel="prefetch" href="/assets/js/43.ad542afd.js"><link rel="prefetch" href="/assets/js/44.65e4feca.js"><link rel="prefetch" href="/assets/js/45.e42086b6.js"><link rel="prefetch" href="/assets/js/46.ce99e06f.js"><link rel="prefetch" href="/assets/js/47.c1b9ed16.js"><link rel="prefetch" href="/assets/js/48.ec583a49.js"><link rel="prefetch" href="/assets/js/49.6dca78f0.js"><link rel="prefetch" href="/assets/js/5.2bb799e1.js"><link rel="prefetch" href="/assets/js/50.73bf2b29.js"><link rel="prefetch" href="/assets/js/51.9191554b.js"><link rel="prefetch" href="/assets/js/52.dfa7fd1f.js"><link rel="prefetch" href="/assets/js/53.0fb52328.js"><link rel="prefetch" href="/assets/js/54.3c407379.js"><link rel="prefetch" href="/assets/js/55.1eaff3bc.js"><link rel="prefetch" href="/assets/js/56.d05203f6.js"><link rel="prefetch" href="/assets/js/57.fcc71dfe.js"><link rel="prefetch" href="/assets/js/58.06a08ea1.js"><link rel="prefetch" href="/assets/js/59.fc248e96.js"><link rel="prefetch" href="/assets/js/6.156409b9.js"><link rel="prefetch" href="/assets/js/60.93032979.js"><link rel="prefetch" href="/assets/js/61.179927ea.js"><link rel="prefetch" href="/assets/js/62.3660011c.js"><link rel="prefetch" href="/assets/js/63.8bcc9217.js"><link rel="prefetch" href="/assets/js/64.b7e3b61f.js"><link rel="prefetch" href="/assets/js/65.8c192055.js"><link rel="prefetch" href="/assets/js/66.3da8bd63.js"><link rel="prefetch" href="/assets/js/67.7ec760ff.js"><link rel="prefetch" href="/assets/js/68.efbc8772.js"><link rel="prefetch" href="/assets/js/69.97a52968.js"><link rel="prefetch" href="/assets/js/7.9135de33.js"><link rel="prefetch" href="/assets/js/70.bb0e8e29.js"><link rel="prefetch" href="/assets/js/71.f5b86ab5.js"><link rel="prefetch" href="/assets/js/72.20084df3.js"><link rel="prefetch" href="/assets/js/73.fc4c82f8.js"><link rel="prefetch" href="/assets/js/74.4e64c65e.js"><link rel="prefetch" href="/assets/js/75.81918bd4.js"><link rel="prefetch" href="/assets/js/76.88d845f4.js"><link rel="prefetch" href="/assets/js/77.cbe81b80.js"><link rel="prefetch" href="/assets/js/78.46210465.js"><link rel="prefetch" href="/assets/js/79.c3ab36b4.js"><link rel="prefetch" href="/assets/js/8.950ca1e5.js"><link rel="prefetch" href="/assets/js/80.2096c587.js"><link rel="prefetch" href="/assets/js/81.4d78b31b.js"><link rel="prefetch" href="/assets/js/82.501bd6bf.js"><link rel="prefetch" href="/assets/js/83.4aa7ac18.js"><link rel="prefetch" href="/assets/js/84.71003c31.js"><link rel="prefetch" href="/assets/js/85.c685bcd8.js"><link rel="prefetch" href="/assets/js/86.fbd67b53.js"><link rel="prefetch" href="/assets/js/87.a6c63be4.js"><link rel="prefetch" href="/assets/js/88.35821686.js"><link rel="prefetch" href="/assets/js/89.1d3cf7b7.js"><link rel="prefetch" href="/assets/js/9.76526246.js"><link rel="prefetch" href="/assets/js/90.eef6b4f9.js"><link rel="prefetch" href="/assets/js/91.a1c9f1cb.js"><link rel="prefetch" href="/assets/js/92.92c28252.js"><link rel="prefetch" href="/assets/js/93.8915f871.js"><link rel="prefetch" href="/assets/js/94.df66423f.js"><link rel="prefetch" href="/assets/js/95.b659290c.js"><link rel="prefetch" href="/assets/js/96.ea314807.js"><link rel="prefetch" href="/assets/js/97.94ef520e.js"><link rel="prefetch" href="/assets/js/98.e9253954.js"><link rel="prefetch" href="/assets/js/99.3ff82af9.js">
    <link rel="stylesheet" href="/assets/css/0.styles.a632c044.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu have-body-img"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/logo.webp" alt="LiuShuixiuのBlog" class="logo"> <span class="site-name can-hide">LiuShuixiuのBlog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/web/" class="nav-link">学习</a></div><div class="nav-item"><a href="/technology/" class="nav-link">技术</a></div><div class="nav-item"><a href="/my/" class="nav-link">我的</a></div><div class="nav-item"><a href="/archives/" class="nav-link">归档</a></div> <a href="https://github.com/LiuShuixiu" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="/img/anno.webp"> <div class="blogger-info"><h3>刘水秀</h3> <span>春风若有怜花意，可否许我再少年</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/web/" class="nav-link">学习</a></div><div class="nav-item"><a href="/technology/" class="nav-link">技术</a></div><div class="nav-item"><a href="/my/" class="nav-link">我的</a></div><div class="nav-item"><a href="/archives/" class="nav-link">归档</a></div> <a href="https://github.com/LiuShuixiu" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>借鉴</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>My</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading open"><span>深度学习</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/2a589a/" class="sidebar-link">线性代数</a></li><li><a href="/pages/62432c/" aria-current="page" class="active sidebar-link">数值稳定性和模型初始化</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/pages/62432c/#_1-梯度消失和梯度爆炸" class="sidebar-link">1.梯度消失和梯度爆炸</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/62432c/#_1-1-梯度消失" class="sidebar-link">1.1 梯度消失</a></li><li class="sidebar-sub-header level3"><a href="/pages/62432c/#_2-梯度爆炸" class="sidebar-link">2\. 梯度爆炸</a></li><li class="sidebar-sub-header level3"><a href="/pages/62432c/#_1-3-打破对称性" class="sidebar-link">1.3 打破对称性</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/62432c/#_2-参数初始化" class="sidebar-link">2\. 参数初始化</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/62432c/#_2-1-默认初始化" class="sidebar-link">2.1 默认初始化</a></li><li class="sidebar-sub-header level3"><a href="/pages/62432c/#_2-2-xavier初始化" class="sidebar-link">2.2 Xavier初始化</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/62432c/#小结" class="sidebar-link">小结</a></li></ul></li><li><a href="/pages/fce000/" class="sidebar-link">暂退法(Dropout)</a></li><li><a href="/pages/dfe5a5/" class="sidebar-link">权重衰减</a></li><li><a href="/pages/a5602b/" class="sidebar-link">模型选择、欠拟合和过拟合</a></li></ul></section></li><li><a href="/note/git/" class="sidebar-link">《Git》学习笔记</a></li></ul></section></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06970110><div class="articleInfo" data-v-06970110><ul class="breadcrumbs" data-v-06970110><li data-v-06970110><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-06970110></a></li> <li data-v-06970110><span data-v-06970110>学习笔记</span></li><li data-v-06970110><span data-v-06970110>My</span></li><li data-v-06970110><span data-v-06970110>深度学习</span></li></ul> <div class="info" data-v-06970110><div title="作者" class="author iconfont icon-touxiang" data-v-06970110><a href="https://github.com/liushuixiu" target="_blank" title="作者" class="beLink" data-v-06970110>LiuShuixiu</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06970110><a href="javascript:;" data-v-06970110>2025-03-15</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">数值稳定性和模型初始化<!----></h1> <!----> <div class="theme-vdoing-content content__default"><p>初始化方案的选择在神经网络学习中起着举足轻重的作用， 它对保持数值稳定性至关重要。 此外，这些初始化方案的选择可以与非线性激活函数的选择有趣的结合在一起。 我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快。 糟糕选择可能会导致我们在训练时遇到<strong>梯度爆炸或梯度消失</strong>。</p> <h2 id="_1-梯度消失和梯度爆炸"><a href="#_1-梯度消失和梯度爆炸" class="header-anchor">#</a> 1.梯度消失和梯度爆炸</h2> <p>考虑一个具有L层、输入x和输出o的深层网络。每一层l由变换fl定义，该变换的参数为权重W(l)， 其隐藏变量是h(l)（令 h(0)=x）。我们的网络可以表示为：</p> <p>因此h(l)=fl(h(l−1)) 因此 o=fL∘…∘f1(x).</p> <p>如果所有隐藏变量和输入都是向量，我们可以将o关于任何一组参数W(l)的梯度写为下式：</p> <p>∂W(l)o=∂h(L−1)h(L)⏟M(L)=def⋅…⋅∂h(l)h(l+1)⏟M(l+1)=def∂W(l)h(l)⏟v(l)=def.</p> <p>换言之，该梯度是L−l个矩阵M(L)⋅…⋅M(l+1)与梯度向量 v(l)的乘积。因此，我们容易受到<strong>数值下溢</strong>问题的影响.当将太多的概率乘在一起时，这些问题经常会出现。在处理概率时，一个常见的技巧是切换到对数空间，即将数值表示的压力从尾数转移到指数。不幸的是，上面的问题更为严重：最初，矩阵 M(l) 可能具有各种各样的特征值。他们可能很小，也可能很大；他们的乘积可能非常大，也可能非常小。</p> <p>不稳定梯度带来的风险不止在于数值表示；不稳定梯度也威胁到我们优化算法的稳定性。我们可能面临一些问题。要么是_梯度爆炸_（gradient exploding）问题：参数更新过大，破坏了模型的稳定收敛；要么是_梯度消失_（gradient vanishing）问题：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</p> <h3 id="_1-1-梯度消失"><a href="#_1-1-梯度消失" class="header-anchor">#</a> 1.1 梯度消失</h3> <p>曾经sigmoid函数1/(1+exp⁡(−x))（ <a href="https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/mlp.html#sec-mlp" target="_blank" rel="noopener noreferrer">4.1节 (opens new window)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>提到过）很流行， 因为它类似于阈值函数。 由于早期的人工神经网络受到生物神经网络的启发， 神经元要么完全激活要么完全不激活（就像生物神经元）的想法很有吸引力。 然而，它却是导致梯度消失问题的一个常见的原因.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>%matplotlib inline
import torch
from d2l import torch as d2l

x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.sigmoid(x)
y.backward(torch.ones_like(x))

d2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()],
         legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5)) 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>1<br>
2<br>
3<br>
4<br>
5<br>
6<br>
7<br>
8<br>
9<br>
10</p> <p><img src="https://liushuixiu.github.io/assets/img/1710253209456.31d649f6.png" alt=""></p> <p>正如上图，当sigmoid函数的输入很大或是很小时，它的梯度都会消失。 此外，当反向传播通过许多层时，除非我们在刚刚好的地方， 这些地方sigmoid函数的输入接近于零，否则整个乘积的梯度可能会消失。 当我们的网络有很多层时，除非我们很小心，否则在某一层可能会切断梯度。 事实上，这个问题曾经困扰着深度网络的训练。 因此，更稳定的<strong>ReLU</strong>系列函数已经成为从业者的默认选择（虽然在神经科学的角度看起来不太合理）。</p> <h3 id="_2-梯度爆炸"><a href="#_2-梯度爆炸" class="header-anchor">#</a> 2. 梯度爆炸</h3> <p>相反，梯度爆炸可能同样令人烦恼。 为了更好地说明这一点，我们生成100个高斯随机矩阵，并将它们与某个初始矩阵相乘。 对于我们选择的尺度（方差σ2=1），矩阵乘积发生爆炸。 当这种情况是由于深度网络的初始化所导致时，我们没有机会让梯度下降优化器收敛。</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>M = torch.normal(0, 1, size=(4,4))
print('一个矩阵 \n',M)
for i in range(100):
    M = torch.mm(M,torch.normal(0, 1, size=(4, 4)))

print('乘以100个矩阵后\n', M) 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>1<br>
2<br>
3<br>
4<br>
5<br>
6</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>结果
一个矩阵 
 tensor([[-0.7872,  2.7090,  0.5996, -1.3191],
        [-1.8260, -0.7130, -0.5521,  0.1051],
        [ 1.1213,  1.0472, -0.3991, -0.3802],
        [ 0.5552,  0.4517, -0.3218,  0.5214]])
乘以100个矩阵后
 tensor([[-2.1897e+26,  8.8308e+26,  1.9813e+26,  1.7019e+26],
        [ 1.3110e+26, -5.2870e+26, -1.1862e+26, -1.0189e+26],
        [-1.6008e+26,  6.4559e+26,  1.4485e+26,  1.2442e+26],
        [ 3.0943e+25, -1.2479e+26, -2.7998e+25, -2.4050e+25]]) 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>1<br>
2<br>
3<br>
4<br>
5<br>
6<br>
7<br>
8<br>
9<br>
10<br>
11</p> <h3 id="_1-3-打破对称性"><a href="#_1-3-打破对称性" class="header-anchor">#</a> 1.3 打破对称性</h3> <p>神经网络设计中的另一个问题是其参数化所固有的对称性。 假设我们有一个简单的多层感知机，它有一个隐藏层和两个隐藏单元。 在这种情况下，我们可以对第一层的权重（）w（1）行重排列， 并且同样对输出层的权重进行重排列，可以获得相同的函数。 第一个隐藏单元与第二个隐藏单元没有什么特别的区别。 换句话说，我们在每一层的隐藏单元之间具有排列对称性。</p> <p>假设输出层将上述两个隐藏单元的多层感知机转换为仅一个输出单元。想象一下，如果我们将隐藏层的所有参数初始化为W(1)=c，c为常量，会发生什么？在这种情况下，在前向传播期间，两个隐藏单元采用相同的输入和参数，产生相同的激活，该激活被送到输出单元。在反向传播期间，根据参数W(1)对输出单元进行微分，得到一个梯度，其元素都取相同的值。因此，在基于梯度的迭代（例如，小批量随机梯度下降）之W(1)的所有元素仍然采用相同的值。这样的迭代永远不会打破对称性，我们可能永远也无法实现网络的表达能力。隐藏层的行为就好像只有一个单元。请注意，虽然小批量随机梯度下降不会打破这种对称性，但<strong>暂退法正则化</strong>可以。</p> <h2 id="_2-参数初始化"><a href="#_2-参数初始化" class="header-anchor">#</a> 2. 参数初始化</h2> <p>解决（或至少减轻）上述问题的一种方法是进行参数初始化， 优化期间的注意和适当的正则化也可以进一步提高稳定性。</p> <h3 id="_2-1-默认初始化"><a href="#_2-1-默认初始化" class="header-anchor">#</a> 2.1 默认初始化</h3> <p>我们使用正态分布来默认初始化权重值。如果我们不指定初始化方法， 框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。</p> <h3 id="_2-2-xavier初始化"><a href="#_2-2-xavier初始化" class="header-anchor">#</a> 2.2 Xavier初始化</h3> <p>让我们看看某些_没有非线性_的全连接层输出（例如，隐藏变量）oi的尺度分布。对于该层nin输入xj及其相关权重wij，输出由下式给出</p> <p>oi=∑j=1ninwijxj.</p> <p>权重wij都是从同一分布中独立抽取的。此外，让我们假设该分布具有零均值和方差σ2。请注意，这并不意味着分布必须是高斯的，只是均值和方差需要存在。现在，让我们假设层xj的输入也具有零均值和方差γ2，并且它们独立于wij并且彼此独立。在这种情况下，我们可以按如下方式计算oi的平均值和方差：</p> <p>E[oi]=∑j=1ninE[wijxj]=∑j=1ninE[wij]E[xj]=0,Var[oi]=E[oi2]−(E[oi])2=∑j=1ninE[wij2xj2]−0=∑j=1ninE[wij2]E[xj2]=ninσ2γ2.</p> <p>保持方差不变的一种方法是设置ninσ2=1。现在考虑反向传播过程，我们面临着类似的问题，尽管梯度是从更靠近输出的层传播的。使用与前向传播相同的推断，我们可以看到，除非noutσ2=1，否则梯度的方差可能会增大，其中nout是该层的输出的数量。这使得我们进退两难：我们不可能同时满足这两个条件。 相反，我们只需满足：</p> <p>或等价于12(nin+nout)σ2=1 或等价于 σ=2nin+nout.</p> <p>这就是现在标准且实用的_Xavier初始化_的基础.通常，Xavier初始化从均值为零，方差 σ2=2nin+nout的高斯分布中采样权重。我们也可以将其改为选择从均匀分布中抽取权重时的方差。 注意均匀分布U(−a,a)的方差为a23。将a23代入到σ2的条件中，将得到初始化值域：</p> <p>U(−6nin+nout,6nin+nout).</p> <p>尽管在上述数学推理中，“不存在非线性”的假设在神经网络中很容易被违反，但Xavier初始化方法在实践中被证明是有效的。</p> <h2 id="小结"><a href="#小结" class="header-anchor">#</a> 小结</h2> <ul><li>梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。</li> <li>需要用启发式的初始化方法来确保初始梯度既不太大也不太小。</li> <li>ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</li> <li>随机初始化是保证在进行优化前打破对称性的关键。</li> <li>Xavier初始化表明，对于每一层，输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响。</li></ul></div></div> <!----> <div class="page-edit"><!----> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/03/15, 18:48:43</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/pages/2a589a/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">线性代数</div></a> <a href="/pages/fce000/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">暂退法(Dropout)</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/pages/2a589a/" class="prev">线性代数</a></span> <span class="next"><a href="/pages/fce000/">暂退法(Dropout)</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/archives/" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/pages/ce3b06/"><div>
            Suwayomi漫画（电脑版）
            <!----></div></a> <span class="date">03-23</span></dt></dl><dl><dd>02</dd> <dt><a href="/pages/45f3b8/"><div>
            常用软件的安装
            <!----></div></a> <span class="date">03-23</span></dt></dl><dl><dd>03</dd> <dt><a href="/pages/7c4c5c/"><div>
            Mihon漫画
            <!----></div></a> <span class="date">03-16</span></dt></dl> <dl><dd></dd> <dt><a href="/archives/" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="mailto:894072666@qq.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/LiuShuixiu" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="https://music.163.com/#/my/m/music/playlist?id=577676682" title="听音乐" target="_blank" class="iconfont icon-erji"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2023-2025
    <span>LiuShuixiu | <a href="https://github.com/xugaoyi/vuepress-theme-vdoing/blob/master/LICENSE" target="_blank">MIT License</a> </span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <div class="body-bg" style="background:url() center center / cover no-repeat;opacity:0.5;"></div> <!----> <!----></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.e2570dae.js" defer></script><script src="/assets/js/2.c541ba58.js" defer></script><script src="/assets/js/3.4de99684.js" defer></script><script src="/assets/js/19.b97bc82f.js" defer></script>
  </body>
</html>
