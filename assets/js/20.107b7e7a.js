(window.webpackJsonp=window.webpackJsonp||[]).push([[20],{375:function(t,_,r){"use strict";r.r(_);var e=r(14),v=Object(e.a)({},(function(){var t=this,_=t._self._c;return _("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[_("blockquote",[_("p",[_("a",{attrs:{href:"https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/dropout.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("4.6. 暂退法（Dropout） — 动手学深度学习 2.0.0 documentation (d2l.ai) (opens new window)"),_("OutboundLink")],1)])]),t._v(" "),_("h2",{attrs:{id:"_1-重新审视过拟合"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-重新审视过拟合"}},[t._v("#")]),t._v(" 1.重新审视过拟合")]),t._v(" "),_("p",[t._v("当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。 不幸的是，线性模型泛化的可靠性是有代价的。 简单地说，"),_("strong",[t._v("线性模型没有考虑到特征之间的交互作用")]),t._v("。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。")]),t._v(" "),_("p",[t._v("泛化性和灵活性之间的这种基本权衡被描述为_偏差-方差权衡_（bias-variance tradeoff）。 线性模型有很高的偏差：它们只能表示一小类函数。 然而，这些模型的方差很低：它们在不同的随机数据样本上可以得出相似的结果。")]),t._v(" "),_("p",[t._v("深度神经网络位于偏差-方差谱的另一端。 与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。 例如，神经网络可能推断“尼日利亚”和“西联汇款”一起出现在电子邮件中表示垃圾邮件， 但单独出现则不表示垃圾邮件。")]),t._v(" "),_("p",[t._v("深度网络的泛化性质令人费解，而这种泛化性质的数学基础仍然是悬而未决的研究问题。")]),t._v(" "),_("h2",{attrs:{id:"_2-扰动的稳健性"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_2-扰动的稳健性"}},[t._v("#")]),t._v(" 2.扰动的稳健性")]),t._v(" "),_("p",[t._v("简单性的另一个角度是平滑性，即函数不应该对其输入的微小变化敏感。 例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。 1995年，克里斯托弗·毕晓普证明了 具有输入噪声的训练等价于Tikhonov正则化 ("),_("a",{attrs:{href:"https://zh-v2.d2l.ai/chapter_references/zreferences.html#id9",target:"_blank",rel:"noopener noreferrer"}},[t._v("Bishop, 1995 (opens new window)"),_("OutboundLink")],1),t._v(")。 这项工作用数学证实了“要求函数光滑”和“要求函数对输入的随机噪声具有适应性”之间的联系。")]),t._v(" "),_("p",[t._v("然后在2014年，斯里瓦斯塔瓦等人 ("),_("a",{attrs:{href:"https://zh-v2.d2l.ai/chapter_references/zreferences.html#id155",target:"_blank",rel:"noopener noreferrer"}},[t._v("Srivastava "),_("em",[t._v("et al.")]),t._v(", 2014 (opens new window)"),_("OutboundLink")],1),t._v(") 就如何将毕晓普的想法应用于网络的内部层提出了一个想法： 在训练过程中，他们建议在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。")]),t._v(" "),_("p",[t._v("这个想法被称为*_"),_("em",[t._v("暂退法***（dropout）。 暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。 这种方法之所以被称为_暂退法")]),t._v("，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。")]),t._v(" "),_("p",[t._v("那么关键的挑战就是如何注入这种噪声。 一种想法是以一种_无偏向_（unbiased）的方式注入噪声。 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。")]),t._v(" "),_("p",[t._v("在毕晓普的工作中，他将高斯噪声添加到线性模型的输入中。 在每次训练迭代中，他将从均值为零的分布ϵ∼N(0,σ2)采样噪声添加到输入x，从而产生扰动点x′=x+ϵ，预期是E[x′]=x。")]),t._v(" "),_("p",[t._v("在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值h以_暂退概率_p由随机变量h′替换，如下所示：")]),t._v(" "),_("p",[t._v("概率为其他情况h′={0 概率为 ph1−p 其他情况")]),t._v(" "),_("p",[t._v("根据此模型的设计，其期望值保持不变，即E[h′]=h。")]),t._v(" "),_("h2",{attrs:{id:"_3-实践中的暂退法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_3-实践中的暂退法"}},[t._v("#")]),t._v(" 3.实践中的暂退法")]),t._v(" "),_("p",[t._v("下图带有1个隐藏层和5个隐藏单元的多层感知机。当我们将暂退法应用到隐藏层，以p的概率将隐藏单元置为零时，结果可以看作一个只包含原始神经元子集的网络。删除了h2和h5， 因此输出的计算不再依赖于h2或h5，并且它们各自的梯度在执行反向传播时也会消失。这样，输出层的计算不能过度依赖于h1,…,h5的任何一个元素。")]),t._v(" "),_("p",[_("img",{attrs:{src:"https://zh-v2.d2l.ai/_images/dropout2.svg",alt:""}})]),t._v(" "),_("p",[_("strong",[t._v("通常，我们在测试时不用暂退法")]),t._v("。 给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。 然而也有一些例外：一些研究人员在测试时使用暂退法， 用于"),_("strong",[t._v("估计神经网络预测的“不确定性”")]),t._v("： 如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。")]),t._v(" "),_("h2",{attrs:{id:"_4-小结"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_4-小结"}},[t._v("#")]),t._v(" 4.小结")]),t._v(" "),_("ul",[_("li",[_("p",[t._v("暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。")])]),t._v(" "),_("li",[_("p",[t._v("暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。")])]),t._v(" "),_("li",[_("p",[t._v("暂退法将活性值ℎ替换为具有期望值ℎ的随机变量。")])]),t._v(" "),_("li",[_("p",[t._v("暂退法仅在训练期间使用。")]),t._v(" "),_("p",[_("img",{attrs:{src:"https://liushuixiu.github.io/assets/img/1710249976897.fd303426.png",alt:""}})])])])])}),[],!1,null,null,null);_.default=v.exports}}]);